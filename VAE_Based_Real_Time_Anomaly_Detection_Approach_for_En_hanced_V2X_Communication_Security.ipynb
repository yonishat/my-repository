{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuPZW/GixvC/XIcNO5ZZtb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonishat/my-repository/blob/main/VAE_Based_Real_Time_Anomaly_Detection_Approach_for_En_hanced_V2X_Communication_Security.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iif0u3TL-p7z"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "45odj_m5-siW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('PyTorch version:', torch.__version__, '\\n')\n",
        "print('GPU name:', torch.cuda.get_device_name(), '\\n')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device is:', device, '\\n')\n",
        "print('Total number of GPUs:', torch.cuda.device_count())"
      ],
      "metadata": {
        "id": "K8zNq3G6-v1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "file_path = '/content/modified_filtered_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "anomaly_data = data[data['time']>=900].reset_index(drop=True)\n",
        "\n",
        "dataset = data[data['time'] < 900].reset_index(drop=True)\n",
        "\n",
        "# Split the remaining data into training (90%) and testing (10%)\n",
        "train_data = dataset[dataset['time'] < 800].reset_index(drop=True)\n",
        "test_data = dataset[dataset['time'] >= 800].reset_index(drop=True)\n",
        "\n",
        "anomaly_data1 = anomaly_data.drop(columns=[ 'angle','id', 'time')\n",
        "\n",
        "# Drop unwanted columns for training data\n",
        "train_data1 = train_data.drop(columns=[ 'angle','id', 'time')\n",
        "\n",
        "# Normalize other columns using MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "data_normalized_train = min_max_scaler.fit_transform(train_data1)\n",
        "data_normalized_train = pd.DataFrame(data_normalized_train, columns=train_data1.columns)\n",
        "# Process the test data similarly\n",
        "test_data1 = test_data.drop(columns=['angle', 'id', 'time')\n",
        "\n",
        "data_normalized_test = min_max_scaler.transform(test_data1)\n",
        "data_normalized_test = pd.DataFrame(data_normalized_test, columns=test_data1.columns)\n",
        "\n",
        "# Process the anoamly data similarly\n",
        "data_normalized_anomaly = min_max_scaler.transform(anomaly_data1)\n",
        "data_normalized_anomaly = pd.DataFrame(data_normalized_anomaly, columns=anomaly_data1.columns)\n",
        "\n",
        "train_data1.shape, anomaly_data1.shape, test_data1.shape"
      ],
      "metadata": {
        "id": "8rE_GHDY-yXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select the anomaly to inject from the following options**\n",
        "\n",
        "\n",
        "1.   Option 1 shows for constatnt speed offset now, but can be changed to position offset by changing a few details.\n",
        "2.   Option 2 shows for Targeted vehicle speed offset now, but can be changed to position offset by changing a few details.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cyVj8Mr__T9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optinon 1\n",
        "# Define function to inject constant offset anomalies\n",
        "def inject_constant_offset(anomaly_data, anomaly_percentage=0.3, high_value_factor= 0.6):\n",
        "    \"\"\"Inject constant offset anomalies into the middle 30% of the test dataset.\"\"\"\n",
        "    total_rows = len(anomaly_data)\n",
        "    num_anomalies = int(total_rows * anomaly_percentage)\n",
        "    if 'speed' in anomaly_data.columns:\n",
        "        max_speed = anomaly_data['speed'].max()\n",
        "        #max_x = anomaly_data['x'].max()\n",
        "        #max_y = anomaly_data['y'].max()\n",
        "\n",
        "        high_value = max_speed * high_value_factor\n",
        "        #high_value_x = max_x * high_value_factor\n",
        "        #high_value_y = max_y * high_value_factor\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Dataset does not contain a 'speed' column.\")\n",
        "    # Determine the start and end indices for the middle 20% of the dataset\n",
        "    start_index = max(0, (total_rows // 2) - (num_anomalies // 2))  # Ensure start_index is not negative\n",
        "    end_index = min(start_index + num_anomalies, total_rows)  # Ensure end_index is within bounds\n",
        "\n",
        "    # Make sure indices are valid\n",
        "    if start_index >= total_rows:\n",
        "        raise ValueError(\"The calculated start index is out of bounds of the dataset.\")\n",
        "    if end_index > total_rows:\n",
        "        end_index = total_rows\n",
        "\n",
        "    anomaly_indices = np.arange(start_index, end_index)\n",
        "\n",
        "    # Create a copy of the data to modify\n",
        "    data_with_anomalies = anomaly_data.copy()\n",
        "\n",
        "    # Inject constant speed offset anomaly at the selected indices\n",
        "    if 'speed' in data_with_anomalies.columns:\n",
        "        for index in anomaly_indices:\n",
        "\n",
        "            data_with_anomalies.iloc[index, data_with_anomalies.columns.get_loc('speed')] += high_value\n",
        "            #data_with_anomalies.iloc[index, data_with_anomalies.columns.get_loc('x')] += high_value_x\n",
        "            #data_with_anomalies.iloc[index, data_with_anomalies.columns.get_loc('y')] += high_value_y\n",
        "\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Dataset does not contain a 'speed' column.\")\n",
        "\n",
        "    return data_with_anomalies, anomaly_indices\n",
        "\n",
        "# Inject anomalies into the middle 20% of the test dataset\n",
        "anomalous_test_data, test_anomaly_indices_offset = inject_constant_speed_offset(anomaly_data, anomaly_percentage=0.3, high_value_factor=0.6)\n",
        "\n",
        "# Generate anomaly labels for the test data\n",
        "anomaly_labels_test = np.zeros(len(anomaly_data), dtype=int)\n",
        "\n",
        "# Ensure anomaly indices are within the range of test_data length\n",
        "test_anomaly_indices_offset = [i for i in test_anomaly_indices_offset if i < len(anomaly_data)]\n",
        "anomaly_labels_test[test_anomaly_indices_offset] = 1\n",
        "\n",
        "# Display the results\n",
        "print(f\"Labels (0 for normal, 1 for anomalous): {anomaly_labels_test}\")\n",
        "anomaly_labels_test.shape\n"
      ],
      "metadata": {
        "id": "I-IErejQ_Rm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2\n",
        "# Define function to inject_Targeted_vehicle_offset anomalies for a specific vehicle\n",
        "def inject_Targeted_vehicle_offset(anomaly_data, vehicle_id, high_value_factor=0.4):\n",
        "\n",
        "    # Check if required columns exist\n",
        "    if 'speed' not in anomaly_data.columns or 'id' not in anomaly_data.columns:\n",
        "        raise ValueError(\"Dataset must contain 'speed' and 'id' columns.\")\n",
        "\n",
        "    # Filter the dataset for the selected vehicle\n",
        "    vehicle_data = anomaly_data[anomaly_data['id'] == vehicle_id]\n",
        "    if vehicle_data.empty:\n",
        "        raise ValueError(f\"No data found for vehicle ID {vehicle_id}.\")\n",
        "\n",
        "    # Calculate the anomaly magnitude\n",
        "    max_speed = anomaly_data['speed'].max()\n",
        "    #max_x = anomaly_data['x'].max()\n",
        "    #max_y = anomaly_data['y'].max()\n",
        "\n",
        "    high_value = max_speed * high_value_factor\n",
        "    #high_value_x = max_x * high_value_factor\n",
        "    #high_value_y = max_y * high_value_factor\n",
        "\n",
        "    # Get the indices of the selected vehicle's rows\n",
        "    anomaly_indices = vehicle_data.index\n",
        "\n",
        "    # Create a copy of the data to modify\n",
        "    data_with_anomalies = anomaly_data.copy()\n",
        "\n",
        "    # Inject anomalies at the selected indices\n",
        "    data_with_anomalies.loc[anomaly_indices, 'speed'] += high_value\n",
        "    #data_with_anomalies.loc[anomaly_indices, 'x'] += high_value_x\n",
        "    #data_with_anomalies.loc[anomaly_indices, 'y'] += high_value_y\n",
        "    return data_with_anomalies, anomaly_indices\n",
        "\n",
        "# Example usage\n",
        "vehicle_id_to_anomaly = \"veh611\" # Replace with the desired vehicle ID\n",
        "anomalous_test_data, test_anomaly_indices_vehicle = inject_constant_speed_offset_vehicle( anomaly_data, vehicle_id=vehicle_id_to_anomaly, high_value_factor=0.6)\n",
        "\n",
        "# Generate anomaly labels for the test data\n",
        "anomaly_labels_test = np.zeros(len(anomaly_data), dtype=int)\n",
        "anomaly_labels_test[test_anomaly_indices_vehicle] = 1\n",
        "\n",
        "# Display the results\n",
        "print(f\"Injected anomalies for vehicle ID {vehicle_id_to_anomaly}:\")\n",
        "print(f\"Labels (0 for normal, 1 for anomalous): {anomaly_labels_test}\")\n",
        "print(f\"Shape of labels: {anomaly_labels_test.shape}\")\n"
      ],
      "metadata": {
        "id": "N2sOci1vAsUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the Unneccesary columns as we did for train and test data\n",
        "anomalous_test_data = anomalous_test_data.drop(columns=['angle', 'id', 'time'])\n",
        "\n",
        "anomalous_normalized_test = min_max_scaler.transform(anomalous_test_data)\n",
        "anomalous_normalized_test = pd.DataFrame(anomalous_normalized_test, columns=anomalous_test_data.columns)\n",
        "\n",
        "anomalous_normalized_test.head(),"
      ],
      "metadata": {
        "id": "gFE2VjXlB6jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply Sliding** to all four data sets and the lables"
      ],
      "metadata": {
        "id": "UfQWOVaTCzKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define the sliding window function\n",
        "def sliding_window(data, window_size, stride):\n",
        "    windows = []\n",
        "    for i in range(0, len(data) - window_size + 1, stride):\n",
        "        window = data.iloc[i:i + window_size].values\n",
        "        windows.append(window)\n",
        "    return np.array(windows)\n",
        "\n",
        "# Parameters for the sliding window\n",
        "window_size = 4  # Number of time steps in each window\n",
        "stride = 1  # Stride to move the window (1 for maximum overlap)\n",
        "\n",
        "# Apply the sliding window to the dataset\n",
        "data_windows = sliding_window(data_normalized_train, window_size, stride)\n",
        "test_condition_windows = sliding_window(data_normalized_test, window_size, stride)\n",
        "anomalous_test_windows = sliding_window(anomalous_normalized_test, window_size, stride)\n",
        "anomalous_normal_windows = sliding_window(data_normalized_anomaly, window_size, stride)\n",
        "\n",
        "# Reshape to add a channel dimension (samples, time_steps, features, channels)\n",
        "data_windows_reshaped = data_windows.reshape(data_windows.shape[0],1, data_windows.shape[1], data_windows.shape[2])\n",
        "anomalous_windows_reshaped = anomalous_test_windows.reshape(anomalous_test_windows.shape[0],1, anomalous_test_windows.shape[1], anomalous_test_windows.shape[2])\n",
        "test_windows_reshaped = test_condition_windows.reshape(test_condition_windows.shape[0],1, test_condition_windows.shape[1], test_condition_windows.shape[2])\n",
        "anomalous_normal_reshaped = anomalous_normal_windows.reshape(anomalous_normal_windows.shape[0],1, anomalous_normal_windows.shape[1], anomalous_normal_windows.shape[2])\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "data_windows_tensor = torch.tensor(data_windows_reshaped, dtype=torch.float32)\n",
        "test_windows_tensor = torch.tensor(test_windows_reshaped, dtype=torch.float32)\n",
        "anomalous_test_windows_tensor = torch.tensor(anomalous_windows_reshaped, dtype=torch.float32)\n",
        "anomalous_normal_windows_tensor = torch.tensor(anomalous_normal_reshaped, dtype=torch.float32)\n",
        "\n",
        "# Print shapes\n",
        "print(\"Data Windows Tensor Shape:\", data_windows_tensor.shape)\n",
        "print(\"Test Condition Windows Tensor Shape:\", test_windows_tensor.shape)\n",
        "print(\"Anomalous Test Windows Tensor Shape:\", anomalous_test_windows_tensor.shape)\n",
        "print(\"Anomalous Test Windows Tensor Shape:\", anomalous_normal_windows_tensor.shape)"
      ],
      "metadata": {
        "id": "x2h3iXfpC61d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_window_labels(labels, window_size, stride):\n",
        "    windows = []\n",
        "    for i in range(0, len(labels) - window_size + 1, stride):\n",
        "        window = labels[i:i + window_size]\n",
        "\n",
        "        windows.append(int(np.any(window)))\n",
        "    return np.array(windows)\n",
        "\n",
        "\n",
        "window_size = 4  # Match your sliding window size\n",
        "stride =1  # Match your sliding window stride\n",
        "anomalous_labels_windows = sliding_window_labels(anomaly_labels_test, window_size, stride)\n",
        "anomalous_labels_windows_tensor = torch.tensor(anomalous_labels_windows, dtype=torch.float32)\n",
        "\n",
        "print(\"Windowed Anomalous Labels:\", anomalous_labels_windows_tensor)\n",
        "print(anomalous_labels_windows_tensor.shape)"
      ],
      "metadata": {
        "id": "ZDCQf4yDDL1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Train and Test dataset"
      ],
      "metadata": {
        "id": "m9wOw4fnDb5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataloader = DataLoader(data_windows_tensor, batch_size=32, shuffle= False, drop_last=True)\n",
        "test_dataloader = DataLoader(test_windows_tensor, batch_size=32, shuffle=False, drop_last=True)\n",
        "print('Length of the training dataloader:', len(training_dataloader))\n",
        "print('Length of the test dataloader:', len(test_dataloader))"
      ],
      "metadata": {
        "id": "0BN97eQBDijd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our VAE architecture"
      ],
      "metadata": {
        "id": "Wcxw9zskDoLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv_VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv_VAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential()\n",
        "        self.encoder.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2, stride=1))\n",
        "        self.encoder.add_module('bnorm1', nn.BatchNorm2d(num_features=4))\n",
        "        #self.encoder.add_module('relu1', nn.ReLU(inplace=True))\n",
        "        self.encoder.add_module('relu1', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
        "        self.encoder.add_module('conv2', nn.Conv2d(in_channels=4, out_channels=8, kernel_size=2, stride=1))\n",
        "        self.encoder.add_module('relu2', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
        "        #self.encoder.add_module('relu2', nn.ReLU(inplace=True))\n",
        "        self.encoder.add_module('conv3', nn.Conv2d(in_channels=8, out_channels=16, kernel_size=2, stride=1))\n",
        "        self.encoder.add_module('relu3', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
        "        #self.encoder.add_module('relu3', nn.ReLU(inplace=True))\n",
        "\n",
        "        self._mu = nn.Linear(in_features=16, out_features=4)\n",
        "        self._logvar = nn.Linear(in_features=16, out_features=4)\n",
        "\n",
        "        self.decoder = nn.Sequential()\n",
        "        self.decoder.add_module('tconv3', nn.ConvTranspose2d(in_channels=4, out_channels=8, kernel_size=2, stride=1))\n",
        "        self.decoder.add_module('relu3', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
        "        #self.encoder.add_module('relu3', nn.ReLU(inplace=True))\n",
        "\n",
        "        self.decoder.add_module('tconv2', nn.ConvTranspose2d(in_channels=8, out_channels=16, kernel_size=2, stride=1))\n",
        "        self.decoder.add_module('relu2',nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
        "        #self.encoder.add_module('relu2', nn.ReLU(inplace=True))\n",
        "\n",
        "        self.decoder.add_module('tconv1', nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=2, stride=1))\n",
        "        self.decoder.add_module('sigmoid1', nn.Sigmoid())\n",
        "\n",
        "    def reparameterization(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.rand_like(std)\n",
        "        sampling = mu + (eps * std)\n",
        "        return sampling\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        mu = self._mu(x)\n",
        "        logvar = self._logvar(x)\n",
        "\n",
        "        x = self.reparameterization(mu, logvar)\n",
        "\n",
        "        x = x.view(-1, 4, 1, 1)\n",
        "\n",
        "        return self.decoder(x), mu, logvar"
      ],
      "metadata": {
        "id": "sZe9XGthDs1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Conv_VAE().to(device)\n",
        "summary(model, input_size=(32, 1, 4, 4))"
      ],
      "metadata": {
        "id": "wKZIR7UBDzv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_batch(data, model, optimizer):\n",
        "    model.train()\n",
        "    #data = data.to(device)\n",
        "    recon, mu, logvar = model(data)\n",
        "    loss = loss_function(recon, data, mu, logvar)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Fo5wL-qnD-p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    def test_batch(data, model):\n",
        "        model.eval()\n",
        "        #data = data.to(device)\n",
        "        recon, mu, logvar = model(data)\n",
        "        loss = loss_function(recon, data, mu, logvar)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "JsAlXlBHEBtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(recon, x, mu, logvar, beta = 1.0):\n",
        "    RECON = F.mse_loss(recon, x, reduction='sum') / x.size(0)\n",
        "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return RECON + beta * KLD"
      ],
      "metadata": {
        "id": "AdtPBnXsEEj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_vae = Conv_VAE().to(device)\n",
        "\n",
        "optimizer = optim.Adam(conv_vae.parameters(), lr=0.001, weight_decay=1e-3)"
      ],
      "metadata": {
        "id": "h4yrls17EKS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train model for 100 epochs with earlystop to avoid overfitting**"
      ],
      "metadata": {
        "id": "CsICwzrCEQJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "patience = 20  # Number of epochs to wait for improvement\n",
        "best_loss = float('inf')  # Initialize best loss as infinity\n",
        "epochs_without_improvement = 0  # Counter for early stopping\n",
        "\n",
        "training_loss, test_loss = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    training_losses, test_losses = [], []\n",
        "\n",
        "    # Training loop\n",
        "    for data in training_dataloader:\n",
        "        trng_batch_loss = training_batch(data, conv_vae, optimizer)\n",
        "        training_losses.append(trng_batch_loss.item())\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "\n",
        "    # Validation loop\n",
        "    for data in test_dataloader:\n",
        "        tst_batch_loss = test_batch(data, conv_vae)\n",
        "        test_losses.append(tst_batch_loss.item())\n",
        "    test_per_epoch_loss = np.array(test_losses).mean()\n",
        "\n",
        "    # Append losses\n",
        "    training_loss.append(training_per_epoch_loss)\n",
        "    test_loss.append(test_per_epoch_loss)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if test_per_epoch_loss < best_loss:\n",
        "        best_loss = test_per_epoch_loss  # Update best loss\n",
        "        epochs_without_improvement = 0  # Reset counter\n",
        "    else:\n",
        "        epochs_without_improvement += 1  # Increment counter\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch: {epoch+1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} |   ', end='')\n",
        "        print(f'Test loss: {test_per_epoch_loss:.4f}')\n",
        "\n",
        "    # Early stopping condition\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"Early stopping triggered at epoch {epoch+1}. Best test loss: {best_loss:.4f}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "KOq5lthrENSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "plt.plot(training_loss, 'g-', linewidth=2, label='Training loss')\n",
        "plt.plot(test_loss, 'c--', linewidth=2, label='Test loss')\n",
        "plt.title('Loss curve', fontsize=23)\n",
        "plt.xlabel('No. of epochs', fontsize=17)\n",
        "plt.ylabel('Losses', fontsize=17)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.legend(fontsize=14);"
      ],
      "metadata": {
        "id": "CnEBKVfMElk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "0lUzs_B9Edf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_vae = conv_vae.to(device)  # Move model to the selected device\n",
        "data_windows_tensor = data_windows_tensor.to(device)\n",
        "reconstructions, _, _ = conv_vae(data_windows_tensor)\n",
        "\n",
        "reconstruction_errors = torch.mean((data_windows_tensor - reconstructions) ** 2, dim=[1, 2, 3])\n"
      ],
      "metadata": {
        "id": "HWHO0Ms2EmcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalous_normal_windows_tensor = anomalous_normal_windows_tensor.to(device)\n",
        "reconstructions_test, _, _ = conv_vae(anomalous_normal_windows_tensor)\n",
        "\n",
        "reconstruction_errors_test = torch.mean((anomalous_normal_windows_tensor - reconstructions_test) ** 2, dim=[1, 2, 3])  # Per sample error"
      ],
      "metadata": {
        "id": "W0coYZTsEwRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalous_test_windows_tensor = anomalous_test_windows_tensor.to(device)\n",
        "reconstructions_anomalous, _, _ = conv_vae(anomalous_test_windows_tensor)\n",
        "\n",
        "reconstruction_errors_anomalous = torch.mean((anomalous_test_windows_tensor - reconstructions_anomalous) ** 2, dim=[1, 2, 3])  # Per sample error"
      ],
      "metadata": {
        "id": "B5ptz_6-EzMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Convert PyTorch tensors to NumPy arrays\n",
        "anomalous_labels_windows_numpy = anomalous_labels_windows_tensor.cpu().numpy()\n",
        "reconstruction_errors_anomalous_numpy = reconstruction_errors_anomalous.detach().cpu().numpy()\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(anomalous_labels_windows_numpy, reconstruction_errors_anomalous_numpy)\n",
        "\n",
        "# Compute AUC\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(\"ROC AUC:\", roc_auc)\n",
        "plt.plot(fpr, tpr, label=\"F1-Score\")"
      ],
      "metadata": {
        "id": "C3IfaXC1E4Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "reconstruction_errors_numpy = reconstruction_errors.detach().cpu().numpy()\n",
        "# Compute the 95th percentile of reconstruction errors\n",
        "threshold = np.percentile(reconstruction_errors_numpy, 98)\n",
        "print(\"Threshold (80th percentile):\", threshold)"
      ],
      "metadata": {
        "id": "T8h35GYlE_QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_predictions = reconstruction_errors_anomalous > threshold\n",
        "anomaly_predictions_numpy = anomaly_predictions.cpu().numpy()\n",
        "\n",
        "# Calculate True Positives, False Positives, True Negatives, and False Negatives\n",
        "tp = ((anomaly_predictions_numpy == 1) & (anomalous_labels_windows_numpy == 1)).sum().item()\n",
        "fp = ((anomaly_predictions_numpy == 1) & (anomalous_labels_windows_numpy == 0)).sum().item()\n",
        "tn = ((anomaly_predictions_numpy == 0) & (anomalous_labels_windows_numpy == 0)).sum().item()\n",
        "fn = ((anomaly_predictions_numpy == 0) & (anomalous_labels_windows_numpy == 1)).sum().item()\n",
        "\n",
        "# Precision, Recall, and F1 Score\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Accuracy\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "oNbW6YaiFd0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reconstruction_errors_test_numpy= reconstruction_errors_test.detach().cpu().numpy()\n",
        "reconstruction_errors_anomalous_numpy = reconstruction_errors_anomalous.detach().cpu().numpy()\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(reconstruction_errors_test_numpy, bins=50, alpha=0.5)\n",
        "plt.xlabel(\"Reconstruction Error Normal\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.axvline(x=threshold, color='r', linestyle='--', label=\"Threshold\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(reconstruction_errors_anomalous_numpy, bins=50, alpha=0.5)\n",
        "plt.axvline(x=threshold, color='r', linestyle='--', label=\"Threshold\")\n",
        "plt.legend()\n",
        "#plt.title(\"Reconstruction Error Distribution\")\n",
        "plt.xlabel(\"Reconstruction Error Anomalous\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HeLY0HnCFD4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate our model's real time anoimaly detection performance**"
      ],
      "metadata": {
        "id": "vf-spWQhFG0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import deque\n",
        "import pandas as pd\n",
        "\n",
        "# Load your trained VAE model\n",
        "vae_model = conv_vae  # Replace with your saved model file\n",
        "vae_model.eval()\n",
        "\n",
        "# Fit the scaler on training data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler.fit(train_data1.values)  # Replace with your training dataset\n",
        "\n",
        "# Define sliding window parameters\n",
        "window_size = 4  # Number of time steps in each window\n",
        "stride = 1  # Stride to move the window\n",
        "sliding_window_buffer = deque(maxlen=window_size)\n",
        "\n",
        "# Step 1: Simulate Streaming from Test Data\n",
        "def stream_test_data(test_data, interval=0.1):\n",
        "    \"\"\"\n",
        "    Stream test data row by row, compatible with both DataFrame and NumPy array.\n",
        "    \"\"\"\n",
        "    if isinstance(test_data, pd.DataFrame):\n",
        "        # DataFrame: Use .iloc to iterate over rows\n",
        "        for i in range(len(test_data)):\n",
        "            yield test_data.iloc[i].values  # Yield row as NumPy array\n",
        "            time.sleep(interval)\n",
        "    elif isinstance(test_data, np.ndarray):\n",
        "        # NumPy array: Iterate over rows directly\n",
        "        for row in test_data:\n",
        "            yield row  # Yield row\n",
        "            time.sleep(interval)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported data format. Must be DataFrame or NumPy array.\")\n",
        "\n",
        "# Step 2: Preprocess Each Row\n",
        "def preprocess_row(row):\n",
        "    \"\"\"\n",
        "    Normalize a single row after ensuring it's numeric and compatible with the scaler.\n",
        "    \"\"\"\n",
        "    # Ensure the row is a 1D array and reshape it to match the scaler's input shape\n",
        "    row = np.array(row).reshape(1, -1)  # Reshape to (1, number of features)\n",
        "\n",
        "    # Ensure the number of features matches the scaler's expectations\n",
        "    if row.shape[1] != train_data1.shape[1]:\n",
        "        raise ValueError(f\"The input row has {row.shape[1]} features, \"\n",
        "                         f\"but {train_data1.shape[1]} features are expected.\")\n",
        "\n",
        "    # Normalize the row using the scaler\n",
        "    normalized_row = scaler.transform(row)\n",
        "    return normalized_row[0]  # Return as a 1D array\n",
        "\n",
        "# Step 3: Real-Time Anomaly Detection with Inference Time Measurement\n",
        "def real_time_sliding_window_detection_with_inference_time(test_data, threshold=threshold, interval=0.1):\n",
        "    total_inference_time = 0.0\n",
        "    num_inferences = 0\n",
        "\n",
        "    for row in stream_test_data(test_data, interval):\n",
        "        # Normalize the incoming data row\n",
        "        normalized_row = preprocess_row(row)\n",
        "\n",
        "        # Add normalized row to the sliding window buffer\n",
        "        sliding_window_buffer.append(normalized_row)\n",
        "\n",
        "        # Process when sliding window is full\n",
        "        if len(sliding_window_buffer) == window_size:\n",
        "            # Convert sliding window to NumPy array\n",
        "            window_array = np.array(sliding_window_buffer)\n",
        "\n",
        "            # Reshape for CNN input (samples, channels, time_steps, features)\n",
        "            window_tensor = torch.tensor(window_array.reshape(1, 1, window_array.shape[0], window_array.shape[1]),\n",
        "                                         dtype=torch.float32)\n",
        "            #window_tensor = window_tensor.to(device)\n",
        "\n",
        "            # Measure inference time\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Anomaly detection with the VAE\n",
        "            #reconstructed, mu, logvar = vae_model(window_tensor)\n",
        "            reconstructed = vae_model(window_tensor)\n",
        "            reconstruction_error = torch.mean((window_tensor - reconstructed) ** 2).item()\n",
        "\n",
        "            # Record inference time\n",
        "            end_time = time.time()\n",
        "            inference_time = end_time - start_time\n",
        "            total_inference_time += inference_time\n",
        "            num_inferences += 1\n",
        "\n",
        "            # Check if it's an anomaly\n",
        "            if reconstruction_error > threshold:\n",
        "                print(f\"Anomaly detected! {inference_time:.6f} seconds\")\n",
        "\n",
        "            #else:\n",
        "               #print(f\"Normal data\")\n",
        "\n",
        "            #print(f\"Inference Time: {inference_time:.6f} seconds\")\n",
        "\n",
        "    # Calculate average inference time\n",
        "    average_inference_time = total_inference_time / num_inferences if num_inferences > 0 else 0\n",
        "    print(f\"Average Inference Time: {average_inference_time:.6f} seconds\")\n",
        "    return average_inference_time\n",
        "\n",
        "# Simulate real-time detection with inference time measurement\n",
        "average_inference_time = real_time_sliding_window_detection_with_inference_time(\n",
        "    anomalous_test_data, threshold=threshold, interval=0.1\n",
        ")\n"
      ],
      "metadata": {
        "id": "h-Lu-yzwFXI_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}